
\chapter{Background and relation prediction studies}

\section[Knowledge Graph embedding (KGE)]{Knowledge Graph embedding: Models, Training, Evaluation}

Given a set $\mathcal{E}$ of entities and a set $\mathcal{R}$ of relations, a knowledge graph $\mathcal{G} \subseteq \mathcal{E} \times \mathcal{R} \times \mathcal{E}$ contains a set of subject-predicate-objects triples $(i,k,j)$, where each triplet is considered as a fact (a \textit{positive example/triple}) of the knowledge graph $\mathcal{G}$. A triple represents the relationship $k \in \mathcal{R}$ between subject $i \in \mathcal{E}$ and object $j \in \mathcal{E}$ of that triple. 
\newline

\noindent\textbf{Knowledge Graph Embedding Models}. Generally, KGE models are trained and evaluated in the context of multi-relational link prediction for the knowledge graph \citep{Ruffinelli2020You}. The goal of multi-relational link prediction is to "complete the KG," i.e., predicting the true but unobserved facts based on the information in $\mathcal{G}$. To perform the prediction, a KGE model learns how to represent entities $ i \in \mathcal{E}$ and relations $k \in \mathcal{R}$ as vectors $e_i \in \mathbb{R}^{d_e}$ and $r_k \in \mathbb{R}^{d_r}$ in a low-dimensional vector space respectively. Those vectors are referred to as representation vectors/embedding vectors. The $d_e, d_r \in \mathbb{N}^+$ are the \textit{embedding size} of the embedding vectors.

Each model uses particular score function $s: \mathcal{E} \times \mathcal{R} \times \mathcal{E} \mapsto \mathbb{R}$ to associate a score $s(i,k,j)$ of a triplet with the likelihood that the triple $(i,k,j)$ is in the knowledge graph $\mathcal{G}$. The higher the score of triple is, the more likely the triple considered to be a fact (i.e., a positive triple) in that knowledge graph $\mathcal{G}$. 

The score function of a KGE model takes form $s(i,k,j) = f(e_i, r_k, e_j)$ where $e_i, r_k, e_j$ are embedding vectors of subject, predicate and object respectively. Function $f$ represented the model architecture could be a fixed function or the learned function \citep{Ruffinelli2020You}. 
\newline


\noindent\textbf{Training objectives}. There are three commonly used approaches to train a KGE model: Negative Sampling (NegSamp) \citep{bordes2013translating}, 1vsAll, \citep{lacroix2018canonical} and KvsAll \citep{dettmers2018conve}. The main difference between those three training objectives is in generating the \textit{negative examples}, i.e., the triples that are not considered incorrect compared to their associated true example. 

Given a triple $t = (i,k,j)$ in training data, \textit{NegSamp} generates the (pseudo-) negative triples by randomly replacing the subject, predicate, or object of the triple $t$ with other entities/predicates (and verifying that obtained triples not in KG). While \textit{1vsAll} omits to sample and considers all triples generated by perturbing the subject and object positions as negative examples (even if those tuples exist in the KG). Finally, \textit{KvsAll}\footnote{KvsAll is the term from \citet{Ruffinelli2020You}, the term the used by \citet{dettmers2018conve} is 1-N.} constructs batches from non-empty rows $(i,k,*)$ or $(*,k,j)$ instead of from individual triples, and labels all such triples as either positive (occurs in training data) or negative (otherwise).

In most studies, the negative triples are generated by replacing the subject and object entities from KG and keeping the true relations in the negative examples. We refer this those triples as \textit{negative entity triples}. Besides perturbing subjects and object positions only, we can also generate negative examples by replacing the true relations with another relation in KG. We refer to those triples as \textit{negative relation triples}.

In order to incorporate relation prediction into the commonly-used 1vsAll training objective as proposed by \cite{chen2021relation}, we include triples generated perturbing the predicate positions (negative relation triples) into the set of negative triples of $t$. 
 
In this thesis, we used 1vsAll as the primary training objective, so as to be able to compare with \citet{chen2021relation}'s results. Thus, to distinguish between original 1vsAll and 1vsAll including negative relation examples, throughout the thesis, we will use the term \textbf{\textit{standard training}} to refer to original 1vsAll and \textbf{\textit{hybrid training}} to refer to 1vsAll including negative relation examples. Finally, 1vsAll objective containing only negative examples generated by perturbing the predicate positions is termed as \textbf{\textit{relation training}}.
\newline

\noindent\textbf{Loss function}. There are several loss functions that have been proposed so far. Mean squared error (MSE) is the loss between the score of each triple and its label (positive - 1 or negative - 0). Pairwise margin ranking with hinge loss (MR) is only applicable with 1vsAll and NegSamp because it makes the score of positive triple higher than its negative examples by at least the margin $\eta$. The binary cross-entropy (BCE) loss proposed by \citet{trouillon2016complex} applies the sigmoid to the score of each triple (positive and negative) and uses cross-entropy between the resulting probability and that tripleâ€™s label to calculate the loss. BCE is suitable for multi-class and multi-label classification \citep{Ruffinelli2020You}. \textbf{Finally, cross-entropy (CE) between the model distribution
(softmax distribution over scores) and the data distribution (labels of corresponding triples, normalized
to sum to 1). CE is more suitable for multi-class classification (as in NegSamp and 1vsAll)
but has also been used in the multi-label setting (KvsAll) \citep{Ruffinelli2020You}.}
\newline

\noindent\textbf{Evaluation}. The performance of the model could be determined by evaluating extit{entity prediction} and/or \textit{relation prediction} tasks. Given a triple $(i,k,j)$, the difference between entity prediction and relation prediction would be the model's questions: for entity prediction, the model tries to answer $(i,k,?)$ question - \textit{object prediction} and $(?,k,j)$ question - \textit{subject prediction}. In contrast, the \textit{relation prediction} model tries to answer the $(i,?,j)$ question. 

To do so, all potential answered triples are first ranked by their score, the higher score the triple has, the lower rank the triple gets. All answered triples, except the give triple $(i,k,j)$, that are in training, validation or test data are discarded so that only new predictions are evaluated \citep{Ruffinelli2020You}. The reason for this is those triplets may be ranked above the test triplet, but this should not be counted as an error because those triplets are valid triples, i.e., they are in KG \citep{bordes2013translating}. Finally, the lower the true answer is ranked, the better the model is; thus, mean reciprocal rank (MRR), mean rank (MR) or the average Hits@K are computed. The lower the rank is, the higher MRR and Hits@K are. While the lower the rank is, the lower the mean rank is. From now on, all of metrics will be reported as filtered metrics.

MRR or Hits@K could be computed separately for each of the question types, i.e., subject, object, relation predictions, and we can aggregate them by computing \textit{the micro average} of metrics (Equation \ref{eq:overall MRR}). To simplify, let denote the MRR of subject and object predictions be \textit{\textbf{entity MRR}} and MRR of relation prediction be \textit{\textbf{relation MRR}}. It's the same for Hits@K, i.e., \textit{\textbf{entity Hits@K}} and \textit{\textbf{relation Hits@K}} respectively. The MRR for the subject, object, and relation predictions can be called as \textit{\textbf{overall MRR}}, same for Hits@K, i.e., \textit{\textbf{overall Hits@K}} (Equation \ref{eq:overall Hits@K}). 

We formally define the evaluation metrics overall filtered MRR and overall filtered Hits@K. Given a triple $(i,k,j)$, we denote the rank$(i|k,j)$ as the filtered rank of object $j$, that is the rank of model score $s(i,k,j)$ among the collection of all pseudo-negative object scores $$\{s(i,k,j): i' \in \mathcal{E} \text{ and } (i',k,j) \text{ does not appear in training, validation or test data} \}\text{.}$$

If a tie exists, the final rank is the mean rank of all triplets with the same score as $s(i,k,j)$. Define the rank$(i|k,j)$ likewise and denote the $\mathcal{K}^{\text{exam}}$ the set of exam triples. Then
\begin{equation}
 \begin{split}
\label{eq:overall MRR}
    \text{MRR}_{\text{overall}} =\frac{1}{3|\mathcal{K}^{\text{exam}}|}
    \sum_{(i,k,j)\in\mathcal{K}^{\text{exam}}}  \bigg(
    & \frac{1}{\text{rank}(i|k,j)} + \\
    & \frac{1}{\text{rank}(j|i,k)} + \\
    & \frac{1}{\text{rank}(k|i,j)} \bigg) 
\end{split}
\end{equation}

\begin{equation}
\label{eq:overall Hits@K}
\begin{split}
    \text{Hits@K}_{\text{overall}} =
    \frac{1}{3|\mathcal{K}^{\text{exam}}|}
    \sum_{(i,k,j)\in\mathcal{K}^{\text{exam}}}
    \bigg(
    & \mathbbm{1}(\text{rank}(i|k,j) \leq K) + \\
    & \mathbbm{1}(\text{rank}(j|i,k) \leq K) + \\
    & \mathbbm{1}(\text{rank}(k|i,j) \leq K)
    \bigg) 
\end{split}
\end{equation}
\noindent where $\mathbbm{1}(E)$ is a indicator such that $\mathbbm{1}(E)$ is 1 if $E$ is true while $\mathbbm{1}(E)$ is 0 if $E$ is false.



\section{Literature on relation prediction.}
\noindent\textbf{The insufficiency of relation prediction studies}. There is a vast amount of literature on adopting entity prediction as to the main evaluation task (i.e., models were evaluated on answering $(i,k,?)$ and $(?,k,j)$ questions) and training objectives (i.e., only negative entity examples are included in the negative set) for KGE methods \citep{bordes2013translating, lin2015learning, nickel2016holographic, wang2014knowledge}. Unfortunately, in terms of relation prediction, few studies have been published on studying relation prediction both in training objective (including negative relation examples to negative set) and evaluation protocol (i.e., models were evaluated on answering $(i,?,j)$ question). 

Table \ref{tab:Papers from top conferences} shows ten papers that I considered related to relation prediction. All those papers were published in top conferences, namely AAAI, NAACL, AKBC, etc., and the rank of the conference was consulted from \textit{The Computing Research and Education Association of Australasia (CORE Inc)}\footnote{CORE Inc: http://portal.core.edu.au/conf-ranks/}.

\begin{table}[!htbp]
\centering
\resizebox{\textwidth}{!}
{
\begin{tabular}{@{}clP{0.5\textwidth}ccP{0.3\textwidth}P{0.3\textwidth}@{}}
\toprule
\multicolumn{1}{l}{\textbf{}} &
  \multicolumn{1}{c}{\textbf{Type}} &
  \multicolumn{1}{c}{\textbf{Paper}} &
  \textbf{Year} &
  \textbf{Conference} &
  \textbf{Relation Prediction} &
  \textbf{Motivation} \\ \midrule
\textbf{1} &
  \textbf{Objective} &
  \textbf{Relation Prediction as an Auxiliary Training Objective for Improving Multi-Relational Graph Representations} &
  \textbf{2021} &
  \textbf{AKBC} &
  \textbf{As Auxiliary Training Object} &
  \textbf{To Improve performance on standard Entity Ranking} \\
\textbf{2} &
  \textbf{Evaluation} &
  \textbf{Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings} &
  \textbf{2020} &
  \textbf{ACL} &
  \textbf{Should be included with standard entity ranking} &
  \textbf{A new standard way to evaluate KGE models} \\
3 &
  Application &
  Strong Baselines for Simple Question Answering over Knowledge Graphs with and without Neural Networks &
  2018 &
  NAACL &
  A subtask in Question and Answering problem &
  To Identify the relation being queried in QA \\
\textbf{4} &
  \textbf{Prediction} &
  \textbf{Type-augmented Relation Prediction in Knowledge Graphs} &
  \textbf{2021} &
  \textbf{AAAI} &
  \textbf{Type information and instance-level information are encode}d &
  \textbf{To improve performance on relation prediction} \\
5 &
  Evaluation &
  Representation Learning of Knowledge Graphs with Entity Descriptions &
  2016 &
  AAAI &
  \multirow{6}{*}{\parbox[t]{\linewidth}{\vspace{2cm}A subtask in evaluation protocol}} &
  \multirow{6}{*}{\parbox[t]{\linewidth}{\vspace{2cm}As a part of the graph completion task}} \\
6 &
  Evaluation &
  Representation Learning of Knowledge Graphs with Hierarchical Types &
  2016 &
  IJCAI &
   &
   \\
7 &
  Evaluation &
  Modeling Relation Paths for Representation Learning of Knowledge Bases &
  2015 &
  EMNLP &
   &
   \\
8 &
  Evaluation &
  ProjE: Embedding Projection for Knowledge Graph Completion &
  2017 &
  AAAI &
   &
   \\
9 &
  Evaluation &
  Shared Embedding Based Neural Networks for Knowledge Graph Completion &
  2018 &
  CIKM &
   &
   \\
10 &
  Evaluation &
  KG-BERT: BERT for Knowledge Graph Completion &
  2019 &
  AAAI* &
   &
   \\
% \textbf{11} &
%   \textbf{Prediction} &
%   \textbf{Multi-relational Link Prediction in Heterogeneous Information Networks} &
%   \textbf{2011} &
%   \textbf{IEEE} &
%   \textbf{As a main focus} &
%   \textbf{To predict relationships, uncover relationships that probably exist but have not been observed} \\
% 12 &
%   Evaluation &
%   Reliable Knowledge Graph Path Representation Learning &
%   2020 &
%   IEEE &
%   As a subtask in evaluation protocol &
%   As a typical evaluation tasks \\ 
  \bottomrule
\end{tabular}
}
\caption[List of ten papers from top conferences related to relation prediction]{List of ten papers from top conferences related to relation prediction. \textit{Type}: indicates the where relation prediction was mentioned in the paper with three values: \textit{objective} means in training objective, \textit{evaluation} means in evaluation task, and \textit{prediction} means in model prediction. The \textit{relation prediction} tells what authors have done with relation prediction. \textit{Motivation} shows us the reason why they considered the relation prediction. The \textit{bold} indicates the paper mainly focuses on relation prediction}
\label{tab:Papers from top conferences}
\end{table}


% \citep{Xie_Liu_Jia_Luan_Sun_2016, xie2016representation, lin2015modeling, shi2017proje, guan2018shared, yao2019kg}
As shown in Table \ref{tab:Papers from top conferences}, six out of ten papers (from paper number five to paper number ten) considered relation prediction as an additional task along with entity prediction in the evaluation protocol. For example, \citet{yao2019kg} evaluated their proposed model - KGE-BERT, by performing relation prediction on FB15K dataset along with entity prediction and triple classification. Similarly, \citet{shi2017proje} also performed relation prediction and entity prediction tasks when evaluating the performance of their proposed model - ProjE \citep{NIPS2013_1cecc7a7}. In the same way as other scholars, \citet{Xie_Liu_Jia_Luan_Sun_2016, xie2016representation} also decided to utilize relation and entity prediction as to their two main evaluation tasks for TKRL and DKRL models. In summary, relation prediction is not the primary task to propose new models in most papers; it has often been considered an additional task to evaluate with the entity prediction \citep{chang2020benchmark}.
\newline

\noindent\textbf{Attempts to study relation prediction.} Although the number of papers studying relation prediction is limited, papers one, two, and four have attempted to study the impact of relation prediction on the training objective, evaluation protocol, and even to improve model performance in relation prediction. 

\citet{cui2021type}, authors of the TaRP model, successfully utilized the description of entities and relations to achieve better performance in terms of relation prediction. To select their best models, \citet{cui2021type} used relation Hits@1 as the main metric. In the same year, \citet{chen2021relation}, in an attempt to improve the model performance in entity ranking, suggested incorporating negative relation examples to the negative set instead of only negative entity examples. Finally, \citet{chang2020benchmark} stressed the importance of including the relation prediction task in the evaluation protocol alongside entity ranking. 

Although there is a lack of attention to relation prediction, we still observed some attempts to study the impact, and some scholars tried to propose a new model for relation prediction. 
\newline

\noindent\textbf{Dataset for relation prediction}. Table \ref{tab:Dataset in relation prediction papers} lists the datasets that have been used to evaluate and analyze the performance of the model in relation prediction. As can be seen, FB15K is the most popular dataset, followed by FB15K-237, UMLS, WN18, and WN18RR. The remaining datasets are the extensions of those popular datasets. Further noticed that most popular datasets (i.e., FB15K, FB15K-237, UMLS, WN18, and WN18RR) contain a small number of relations except FB15K and FB15K-237, while the remaining datasets which contain a large number of relation were not really considered in most of studies. 
\newline

\begin{table}[!htbp]
\centering
\resizebox{\textwidth}{!}
{
\begin{tabular}{llccc@{}}
\toprule
\multicolumn{1}{c}{ID} & \multicolumn{1}{c}{Dataset}                                                    & Entities & Relations & Appeared in (papers) \\ \midrule
1  & FB15k \citep{NIPS2013_1cecc7a7}       & 14,951   & \textbf{1,345} & 9 \\
2  & FB15k-237 \citep{toutanova-etal-2015-representing}   & 27,395   & 237   & 3 \\
3  & UMLS \citep{ICML-2007-KokD}       & 135      & 49    & 3 \\
4  & WN18 \citep{dettmers2018conve}       & 40   559 & 18    & 2 \\
5  & WN18RR \citep{dettmers2018conve}     & 40,943   & 11    & 2 \\
6  & SemMedDB    & 1,6M     & \textbf{117K}  & 1 \\
7                      & \begin{tabular}[c]{@{}l@{}}SIMPLEQUESTIONS   \\ (facts from FB2M)\end{tabular} & 45,335   & \textbf{1,703}     & 1           \\
8  & Aristo-v4 \citep{Dalvi2017DomainTargetedHP}  & 44,950   & \textbf{1,605} & 1 \\
9  & FB40K       & 39,528   & \textbf{1,336} & 1 \\
10 & FB20K       & 19,923   & \textbf{1,336} & 1 \\
11 & DB111K-174 \citep{hao2019universal} & 98,336   & 298   & 1 \\
12 & Nations \citep{ICML-2007-KokD}     & 125      & 57    & 1 \\
13 & YAGO26K-906 \citep{hao2019universal} & 26,078   & 34    & 1 \\
14 & Kinships \citep{ICML-2007-KokD}   & 104      & 26    & 1 \\ \bottomrule
\end{tabular}
}
\caption[The dataset used in papers]{The dataset in relation prediction papers with their corresponding number of entities and relations as well as the number of papers used them.}
\label{tab:Dataset in relation prediction papers}
\end{table}


\noindent\textbf{Training objectives and evaluation metrics}. Noticeably, in those ten studies, the most common training objective is NegSamp which contains negative triples generated by randomly replacing the subject, object, or predicate \citep{Xie_Liu_Jia_Luan_Sun_2016, xie2016representation, lin2015modeling, shi2017proje}. Those negative triples are verified not to appear in the KG. Alongside with NegSamp training objective, margin ranking loss was mainly adopted to calculate the loss between each positive triple and its negative examples \citep{Xie_Liu_Jia_Luan_Sun_2016, xie2016representation, lin2015modeling}. 

Despite Mean Rank's shortcomings since it is sensitive to outliers \citep{nickel2016holographic}, MR is the most popular metric used to evaluate the models' performance in relation prediction \citep{cui2021type, Xie_Liu_Jia_Luan_Sun_2016, xie2016representation, lin2015modeling, shi2017proje, guan2018shared, yao2019kg}. Besides, MRR and Hits@\{1, 3, 10\} were also used to evaluate the models' performance \citep{chang2020benchmark, chen2021relation}. Those metrics were reported separately between relation prediction and entity prediction.
\newline

\noindent\textbf{Model selection.} Many studies have decided to select their best models based on filter entity MRR, entity Hits@K, \citep{Xie_Liu_Jia_Luan_Sun_2016, xie2016representation, lin2015modeling, shi2017proje, guan2018shared, yao2019kg}  or relation Hits@K only \citep{cui2021type}. We believe that no other authors have studied the effect of selecting the best model on overall MRR so far.  

\section{Conclusion}

Despite the interest in relation prediction, research has focused on entity ranking rather than relation prediction. Table \ref{tab:Papers from top conferences} demonstrates the lack of studies that truly focus on relation prediction performance. Most papers only considered relation prediction (i.e., models were evaluated on answering $(i,?,j)$ question) as an auxiliary task in model evaluation. Unexpectedly, no paper studies selecting the best models that can achieve the best performance on both relation prediction and entity prediction simultaneously (overall MRR). 

To the best of our knowledge, \citet{chen2021relation} are the first authors who studied the effect of including the negative relation examples alongside negative entity examples in the 1vsAll training objective. Their empirical experiments showed an improvement in entity prediction when including negative relation examples in the 1vsAll training objective. Their paper research might have been more interesting if the relation prediction performance of models had been reported when studying hybrid training.

In the next chapter (Chapter \ref{chap:reproducibility}), to allow us to study the impact of hybrid training on relation prediction performance, we attempted to reproduce their results using LibKGE \citep{libkge} to assure that we have the same models that they had. The reason for this effort is to have fairness in the comparative study: where we comprehensively studied the impact of hybrid training and overall MRR on models' performance in relation prediction tasks (Chapter \ref{chap:comparative_study}). 

